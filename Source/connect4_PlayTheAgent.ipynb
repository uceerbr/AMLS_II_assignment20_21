{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect Four"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the libraries and define global variables (eg the size of the gameboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-23 18:25:43.061901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\randy\\anaconda3\\envs\\amls2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\randy\\anaconda3\\envs\\amls2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\randy\\anaconda3\\envs\\amls2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\randy\\anaconda3\\envs\\amls2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\randy\\anaconda3\\envs\\amls2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\randy\\anaconda3\\envs\\amls2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import datetime\n",
    "print(datetime.datetime.now())\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import scipy.misc\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "global Xdim\n",
    "global Ydim\n",
    "global WinNumber  # Number in a row necessary for a win, typically 4\n",
    "global RewardForWin\n",
    "global MAX\n",
    "MAX = 1e308\n",
    "global game_number_list\n",
    "global mean_reward_list\n",
    "\n",
    "# Connect-4 is usually played on a 7x6 gameboard\n",
    "Xdim = 7\n",
    "action_space_size=Xdim\n",
    "Ydim = 6\n",
    "WinNumber = 4\n",
    "# Define your incentives\n",
    "RewardForWin = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 # How many experiences to use for each training step. Mnih2015=32\n",
    "\n",
    "update_freq = 8   # Actually, this is frequency inverted, the number of steps between training rounds. Mnih2015=4\n",
    "\n",
    "targetQ_y = 0.400  # Discount factor on the target Q-values Mnih2015=0.99\n",
    "\n",
    "pre_train_steps = 500 # How many steps of random actions before training begins\n",
    "startE = 1 # Starting chance of random action Mnih2015=1\n",
    "endE = 0.1 # Final chance of random action Mnih2015=0.1\n",
    "annealing_time = 0.6 # Percent of games to taper off the exploration: 0 = go immediately to endE, 1 = taper exploration during 100% of games, Mnih2015=.02\n",
    "\n",
    "h_size = 512 # The number of units in the hidden layer. Mnih2015=512\n",
    "\n",
    "learning_rate = 0.0003 # Mnih2015 = .00025\n",
    "\n",
    "training_games = 500000 # overnight\n",
    "\n",
    "model_path = \"./Trainings_500Ktraining/models4/dqn_training_reducedLossPenalty\" # The path to save our model to.\n",
    "summary_path = './summaries4/dqn' # The path to save summary statistics to.\n",
    "load_model = True # Whether to load a saved model.\n",
    "train_model = False # Whether to train the model\n",
    "if (train_model==False):\n",
    "    training_games = 1\n",
    "\n",
    "PRETRAIN_WINS = False\n",
    "Pretrain_Scale = 5\n",
    "PRETRAIN_THREATS = False \n",
    "MIRRORED_TRAINING = False\n",
    "REWARD_FOR_THREAT = False\n",
    "MINIMAX_TRAINING = False # Not programmed yet\n",
    "REWARD_EVERY_MOVE = True\n",
    "TRAIN_MISSED_WINS = False\n",
    "TRAIN_MISSED_BLOCKS = False\n",
    "PRIORITIZED_EXPERIENCE = False\n",
    "TRAIN_AFTER_TESTGAMES = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define sub-routines for displaying the board and checking for a win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# DISPLAY THE GAMEBOARD WITH PROPER ORIENTATION\n",
    "#\n",
    "def display_gameboard(gameboard):\n",
    "    ylen = len(gameboard[0])\n",
    "    xlen = len(gameboard)\n",
    "    #print(xlen,ylen)\n",
    "    gameboard_for_display = np.zeros((xlen,ylen),dtype=int)\n",
    "    for i in range(ylen):\n",
    "        gameboard_for_display[:,i] = gameboard[:,ylen-1-i]\n",
    "    print(gameboard_for_display.T)\n",
    "    #print(\"\\n\",gameboard_for_display.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  CHECK TO SEE IF THE LATEST MOVE WINS THE GAME\n",
    "#\n",
    "def check_for_win_vector(vector):\n",
    "    reward = 0\n",
    "    for start in range(0,len(vector)-WinNumber+1):\n",
    "        win_at_this_startpoint = 1\n",
    "        for i in range(start, start+WinNumber):\n",
    "            if (vector[i] != 1):\n",
    "                win_at_this_startpoint = 0\n",
    "        if (win_at_this_startpoint):\n",
    "            reward=1\n",
    "            return(reward)\n",
    "    return(reward)\n",
    "    \n",
    "def check_for_win(gameboard,x,y):\n",
    "    horizontalVector = gameboard.T[y]\n",
    "    #print('horizontalVector=',horizontalVector)\n",
    "    reward=check_for_win_vector(horizontalVector)\n",
    "    if reward:\n",
    "        #print('horizontalVector=',horizontalVector)\n",
    "        return(reward)\n",
    "    \n",
    "    verticalVector = gameboard[x]\n",
    "    #print('verticalVector= ',verticalVector)\n",
    "    reward=check_for_win_vector(verticalVector)\n",
    "    if reward:\n",
    "        #print('verticalVector= ',verticalVector)\n",
    "        return(reward)\n",
    "    \n",
    "    diagonalVectorUp=np.zeros(max(Xdim,Ydim),dtype=int)\n",
    "    diagonalVectorDn=np.zeros(max(Xdim,Ydim),dtype=int)\n",
    "    for i in range(0,max(Xdim,Ydim)):\n",
    "        ycoord = i\n",
    "        xcoordUp = x-(y-i) # create vector running down-left to up-right\n",
    "        if (i < Ydim) and (xcoordUp in range(0,Xdim)):\n",
    "            diagonalVectorUp[i]= gameboard[xcoordUp][ycoord]\n",
    "        xcoordDn = x+(y-i) # create vector running up-left to down-right\n",
    "        if (i < Ydim) and (xcoordDn in range(0,Xdim)):\n",
    "            diagonalVectorDn[-1-i]= gameboard[xcoordDn][ycoord]\n",
    "       \n",
    "    #print('diagonalVectorUp = ',diagonalVectorUp)\n",
    "    reward=check_for_win_vector(diagonalVectorUp)\n",
    "    if reward:\n",
    "        #print('diagonalVectorUp = ',diagonalVectorUp)\n",
    "        return(reward)\n",
    "    \n",
    "    #print('diagonalVectorDn = ',diagonalVectorDn)\n",
    "    reward=check_for_win_vector(diagonalVectorDn)\n",
    "    if reward:\n",
    "        #print('diagonalVectorDn = ',diagonalVectorDn)\n",
    "        return(reward)\n",
    "    \n",
    "    return(0) # reward= 0 if no win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_possible_win(gameboard):\n",
    "    possible_win=0\n",
    "    win_column=0\n",
    "    for column in range(Xdim):\n",
    "        gameboard_possible=gameboard.copy()\n",
    "        if(gameboard[column,-1]==0):\n",
    "            row = list(gameboard[column]).index(0) # Find the lowest open slot in the chosen column\n",
    "            gameboard_possible[column,row] = 1\n",
    "            if check_for_win(gameboard_possible,column,row):\n",
    "                possible_win+=1\n",
    "                win_column=column\n",
    "                #print('\\nPossible win in column', win_column)\n",
    "                #display_gameboard(gameboard_possible)\n",
    "    return(possible_win,win_column)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_possible_win_verbose(gameboard):  #Use this to assist debugging\n",
    "    possible_win=0\n",
    "    win_column=0\n",
    "    for column in range(Xdim):\n",
    "        gameboard_possible=gameboard.copy()\n",
    "        if(gameboard[column,-1]==0):\n",
    "            row = list(gameboard[column]).index(0) # Find the lowest open slot in the chosen column\n",
    "            gameboard_possible[column,row] = 1\n",
    "            if check_for_win(gameboard_possible,column,row):\n",
    "                print('POSSIBLE WIN HERE')\n",
    "                display_gameboard(gameboard_possible)\n",
    "                possible_win+=1\n",
    "                win_column=column\n",
    "                #print('\\nPossible win in column', win_column)\n",
    "                #display_gameboard(gameboard_possible)\n",
    "    return(possible_win,win_column)           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self ,h_size, num_actions, lr, scope):\n",
    "        with tf.variable_scope(scope):\n",
    "            # The network recieves a frame from the game, flattened into an array.\n",
    "            # It then resizes it and processes it through two convolutional layers.\n",
    "            #self.observation_input =  tf.placeholder(shape=[None, 12, 12, 3],dtype=tf.float32)\n",
    "            self.observation_input =  tf.placeholder(shape=[None, Xdim, Ydim, 3],dtype=tf.float32) #new\n",
    "            \n",
    "            self.conv1 = slim.conv2d(self.observation_input, 64, \n",
    "                                     kernel_size=[3,3], stride=[2,2], \n",
    "                                     biases_initializer=None,\n",
    "                                     activation_fn=tf.nn.elu)\n",
    "            self.conv2 = slim.conv2d(self.conv1, 64, \n",
    "                                     kernel_size=[3,3], \n",
    "                                     stride=[2,2], \n",
    "                                     biases_initializer=None,\n",
    "                                     activation_fn=tf.nn.elu)\n",
    "\n",
    "            # We take the output from the final convolutional layer \n",
    "            # and split it into separate advantage and value streams.\n",
    "            self.hidden = slim.fully_connected(slim.flatten(self.conv2), \n",
    "                                               h_size, activation_fn=tf.nn.elu)\n",
    "            self.advantage = slim.fully_connected(self.hidden, num_actions, activation_fn=None,\n",
    "                                                  biases_initializer=None)\n",
    "            self.value = slim.fully_connected(self.hidden, 1, activation_fn=None,\n",
    "                                                  biases_initializer=None)\n",
    "\n",
    "            # Task 1: Combine advantage and vaule together to get the final Q-values.\n",
    "            self.q_out = self.value + tf.subtract(self.advantage, \n",
    "                                                  tf.reduce_mean(self.advantage,axis=1, keep_dims=True))\n",
    "            # Task 2: Select the best action given q_out\n",
    "            self.predict = tf.argmax(self.q_out,1)\n",
    "\n",
    "            # Below we obtain the loss by taking the sum of squares difference \n",
    "            # between the target and prediction Q values.\n",
    "            self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "            self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "            self.actions_onehot = tf.one_hot(self.actions,num_actions,dtype=tf.float32)\n",
    "\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.q_out, self.actions_onehot), axis=1)\n",
    "\n",
    "            # Task 3: Compute the TD error\n",
    "            self.td_error = tf.square(self.targetQ - self.Q)\n",
    "            self.loss = tf.reduce_mean(self.td_error)\n",
    "            self.trainer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "            self.update = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self,experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "            \n",
    "    def sample(self,size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer,size)),[size,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update the target network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target_graph(from_scope, to_scope):\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n",
    "\n",
    "    op_holder = []\n",
    "    for from_var,to_var in zip(from_vars,to_vars):\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "    return op_holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\randy\\anaconda3\\envs\\amls2\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\randy\\anaconda3\\envs\\amls2\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From C:\\Users\\randy\\anaconda3\\envs\\amls2\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Train the network \n",
    "# Definitions\n",
    "action_space_size = Xdim\n",
    "tf.reset_default_graph()\n",
    "mainQN = Qnetwork(h_size, 1, learning_rate, \"main\")\n",
    "targetQN = Qnetwork(h_size, 1, learning_rate, \"target\")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "if not os.path.exists(summary_path):\n",
    "    os.makedirs(summary_path)\n",
    "    \n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "update_target_ops = update_target_graph(\"main\", \"target\")\n",
    "\n",
    "myBuffer = experience_buffer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LET THE USER PLAY AGAINST THE COMPUTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_input(gameboard):\n",
    "    display_gameboard(gameboard)\n",
    "    user_string= input('Pick a column to play (1-7)  ')\n",
    "    while user_string[0] not in ['1','2','3','4','5','6','7']:\n",
    "        display_game_board(gameboard)\n",
    "        print('Illegal entry.  Pick a column (1-7)  ')\n",
    "    column=int(user_string[0])-1\n",
    "    if gameboard[column,-1]:\n",
    "        print(\"That column is already full.  I'll pick for you this time.\")\n",
    "        while gameboard[column,-1]:\n",
    "            column = random.choice(range(Xdim))\n",
    "    return(column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_agent_with_winlosssearch_middlestart(gameboard):    \n",
    "        \n",
    "    # On the first move, start in the middle\n",
    "    if (-1) not in gameboard: \n",
    "        return(int(Xdim/2))  \n",
    "    \n",
    "    # Take a win if you have it\n",
    "    possible_win,win_column=check_for_possible_win(gameboard)\n",
    "    if possible_win:\n",
    "        return(win_column)\n",
    "    \n",
    "    # Make a block if you need it\n",
    "    possible_block,block_column=check_for_possible_win(-1*gameboard)\n",
    "    if possible_block: # Make a block if you need it\n",
    "        return(block_column)\n",
    "    \n",
    "    Qstate_value = [0.0 for i in range(Xdim)]\n",
    "    for column in range(Xdim):\n",
    "        if gameboard[column,-1]:\n",
    "            Qstate_value[column]= -MAX # don't allow an illegal move into a full column\n",
    "        else:\n",
    "            row = list(gameboard[column]).index(0) # Find the lowest open slot in the chosen column #new\n",
    "            gameboard_possible = gameboard.copy()\n",
    "            gameboard_possible[column,row] = 1\n",
    "            possible_loss, loss_column = check_for_possible_win(-1*gameboard_possible)\n",
    "            if (possible_loss): # avoid giving them an opportunity\n",
    "                Qstate_value[column] = -MAX/2\n",
    "            else:\n",
    "                # Default is random\n",
    "                Qstate_value[column]=random.random()\n",
    "                        \n",
    "    column = Qstate_value.index(max(Qstate_value))\n",
    "    return(column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Would you like to play a game against the semi-random opponent? (y/n)y\n",
      "\n",
      "The spots you play will be marked 1\n",
      "The spots the computer plays will be marked -1\n",
      "\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]]\n",
      "Pick a column to play (1-7)  4\n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0 -1  0]]\n",
      "Pick a column to play (1-7)  4\n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [ 0 -1  0  1  0 -1  0]]\n",
      "Pick a column to play (1-7)  4\n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [ 0 -1  0  1  0 -1  0]]\n",
      "Pick a column to play (1-7)  4\n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [-1 -1  0  1  0 -1  0]]\n",
      "Pick a column to play (1-7)  4\n",
      "[[ 0  0  0  1  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [ 0 -1  0  1  0  0  0]\n",
      " [-1 -1  0  1  0 -1  0]]\n",
      "Pick a column to play (1-7)  4\n",
      "That column is already full.  I'll pick for you this time.\n",
      "[[ 0  0  0  1  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [-1 -1  0  1  0  0  0]\n",
      " [-1 -1  0  1  0 -1  1]]\n",
      "Pick a column to play (1-7)  3\n",
      "[[ 0  0  0  1  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [-1 -1  0  1  0  0 -1]\n",
      " [-1 -1  1  1  0 -1  1]]\n",
      "Pick a column to play (1-7)  3\n",
      "[[ 0  0  0  1  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 0  0 -1  1  0  0  0]\n",
      " [-1 -1  1  1  0  0 -1]\n",
      " [-1 -1  1  1  0 -1  1]]\n",
      "Too Bad!\n",
      "Would you like to play a game against the semi-random opponent? (y/n)n\n",
      "OK, we'll play later.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Let the user play a set of games against the semi-random opponent\n",
    "#\n",
    "\n",
    "computer_player = random_agent_with_winlosssearch_middlestart\n",
    "play_yes_no = 'invalid'\n",
    "done=0\n",
    "while (play_yes_no[0] != 'n'):\n",
    "    while (play_yes_no[0] != 'n') and (play_yes_no[0] != 'y'):\n",
    "        play_yes_no = input('Would you like to play a game against the semi-random opponent? (y/n)')\n",
    "    if (play_yes_no[0]== 'y'):\n",
    "        print(\"\\nThe spots you play will be marked 1\\nThe spots the computer plays will be marked -1\\n\")\n",
    "        play_yes_no = 'invalid' # Loop back after the game to see if we play another game\n",
    "        gameboard = np.zeros((Xdim,Ydim),dtype=int) \n",
    "        while (done==0 and 0 in gameboard):\n",
    "            # User turn\n",
    "            user_column = get_user_input(gameboard)\n",
    "            user_row = list(gameboard[user_column]).index(0)\n",
    "            gameboard[user_column,user_row]=1\n",
    "            if check_for_win(gameboard,user_column,user_row):\n",
    "                display_gameboard(gameboard)\n",
    "                print('You Win!')\n",
    "                done=1\n",
    "            gameboard = -1*gameboard\n",
    "            #\n",
    "            # Computer turn\n",
    "            if (done==0 and 0 in gameboard):\n",
    "                computer_column = computer_player(gameboard)\n",
    "                computer_row = list(gameboard[computer_column]).index(0)\n",
    "                gameboard[computer_column,computer_row]=1\n",
    "                if check_for_win(gameboard,computer_column,computer_row):\n",
    "                    display_gameboard(-1*gameboard)\n",
    "                    print('Too Bad!')\n",
    "                    done=1\n",
    "                gameboard = -1*gameboard\n",
    "        \n",
    "        if (done==0 and 0 not in gameboard): # Tie game\n",
    "            display_gameboard(gameboard)\n",
    "            print('Tie game')\n",
    "          \n",
    "print(\"OK, we'll play later.\") # No more games\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Qvalue(gameboard):\n",
    "    observations = np.reshape(gameboard, [gameboard.shape[0], gameboard.shape[1], 1])\n",
    "    observation = np.concatenate([observations, observations, observations], axis=2)\n",
    "    Qvalue = sess.run(mainQN.q_out,feed_dict={mainQN.observation_input:[observation]})[0]\n",
    "    return(Qvalue[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trained_agent_minimax_lookahead2_forced(gameboard):\n",
    "\n",
    "    thresholdA = MAX\n",
    "    Qstate_valueA = [0.0 for i in range(Xdim)]\n",
    "    for columnA in range(Xdim):\n",
    "        gameboardA = gameboard.copy()\n",
    "        if gameboardA[columnA,-1]:\n",
    "            Qstate_valueA[columnA]= -MAX # don't allow an illegal move into a full column\n",
    "        else:\n",
    "            row = list(gameboardA[columnA]).index(0) # Find the lowest open slot in the chosen column\n",
    "            gameboardA[columnA,row] = 1 \n",
    "            if (check_for_win(gameboardA,columnA,row)) or (0 not in gameboardA):\n",
    "                return(columnA) # Force a winning move\n",
    "                Qstate_valueA[columnA] = get_Qvalue(gameboardA)\n",
    "                #if (check_for_win(gameboardA,columnA,row)):\n",
    "                    #Qstate_valueA[columnA] = MAX/2\n",
    "                #break # No need to evaluate deeper if this terminates the game\n",
    "            else:\n",
    "                 \n",
    "                thresholdB = MAX\n",
    "                Qstate_valueB = [0.0 for i in range(Xdim)]\n",
    "                for columnB in range(Xdim):\n",
    "                    gameboardB = -1*gameboardA.copy()\n",
    "                    if gameboardB[columnB,-1]:\n",
    "                        Qstate_valueB[columnB] = -MAX # don't allow an illegal move into a full column\n",
    "                    else:\n",
    "                        row = list(gameboardB[columnB]).index(0) # Find the lowest open slot in the chosen column\n",
    "                        gameboardB[columnB,row] = 1                        \n",
    "                        if (check_for_win(gameboardB,columnB,row)) or (0 not in gameboardB):\n",
    "                            Qstate_valueB[columnB] = get_Qvalue(gameboardB)\n",
    "                            if (check_for_win(gameboardB,columnB,row)):\n",
    "                                Qstate_valueB[columnB] = MAX/3  # Prioritize a block\n",
    "                            #break # No need to evaluate deeper if this terminates the game\n",
    "                        else:\n",
    "                        \n",
    "                            Qstate_valueC = [0.0 for i in range(Xdim)]\n",
    "                            for columnC in range(Xdim):\n",
    "                                gameboardC = -1*gameboardB.copy()\n",
    "                                if gameboardC[columnC,-1]:\n",
    "                                    Qstate_valueC[columnC] = -MAX # don't allow an illegal move into a full column\n",
    "                                else:\n",
    "                                    row = list(gameboardC[columnC]).index(0) # Find the lowest open slot in the chosen column\n",
    "                                    gameboardC[columnC,row] = 1\n",
    "                                    if (check_for_win(gameboardC,columnC,row)):\n",
    "                                        #Qstate_valueC[columnC] = get_Qvalue(gameboardC)\n",
    "                                        Qstate_valueC[columnC] = MAX/4   # Pioritize a win\n",
    "                                    else:\n",
    "                                        Qstate_valueC[columnC]= get_Qvalue(gameboardC)\n",
    "                                # End of loop of each value of columnC \n",
    "                                if (Qstate_valueC[columnC] > thresholdB): # If even one of B's options is higher, we don't need to evaluate further\n",
    "                                    break #PRUNING\n",
    "                            # This runs at the end of the columnC loop        \n",
    "                            #columnCmax = Qstate_valueB.index(max(Qstate_valueC))\n",
    "                            Qstate_valueC_max = max(Qstate_valueC)\n",
    "                            if (Qstate_valueC_max<thresholdB):\n",
    "                                thresholdB = Qstate_valueC_max\n",
    "                            Qstate_valueB[columnB] = -1*Qstate_valueC_max # Invert the opponent's gain to be our loss\n",
    "                    \n",
    "                    # End of loop of each value of columnB\n",
    "                    if (Qstate_valueB[columnB] > thresholdA): # If even one of B's options is higher, we don't need to evaluate further\n",
    "                        break #PRUNING       \n",
    "                \n",
    "                # This runs at the end of the columnB loop of all values of columnB\n",
    "                #columnBmax = Qstate_valueB.index(max(Qstate_valueB)) #opponent maximizes their Qvalue for each option\n",
    "                Qstate_valueB_max = max(Qstate_valueB)\n",
    "                if (Qstate_valueB_max<thresholdA):\n",
    "                    thresholdA = Qstate_valueB_max\n",
    "                Qstate_valueA[columnA] = -1*Qstate_valueB_max # Invert the opponent's gain to be our loss\n",
    "\n",
    "    count = Qstate_valueA.count(max(Qstate_valueA))\n",
    "    if count>1:\n",
    "        columnA = random.randint(0,Xdim-1)\n",
    "        while Qstate_valueA[columnA] != max(Qstate_valueA):\n",
    "            columnA=random.randint(0,Xdim-1)\n",
    "    else:\n",
    "        columnA = Qstate_valueA.index(max(Qstate_valueA)) # Find the first/only occurence of the max value\n",
    "    \n",
    "    return(columnA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\randy\\anaconda3\\envs\\amls2\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./Trainings_500Ktraining/models4/dqn_training_reducedLossPenalty\\model-499999.cptk\n",
      "Model loaded\n",
      "Would you like to play a game against the trained agent? (y/n)n\n",
      "OK, we'll play later.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Let the user play against the trained agent\n",
    "#\n",
    "playerA=  get_user_input\n",
    "playerB= trained_agent_minimax_lookahead2_forced\n",
    "TRAIN_AFTER_TESTGAMES = False\n",
    "\n",
    "\n",
    "#This is up-to-date altered\n",
    "#\n",
    "# Play a set of matches\n",
    "#\n",
    "global player1_victories\n",
    "global player2_victories\n",
    "player1_victories = 0\n",
    "player2_victories = 0\n",
    "playerA_victories = 0\n",
    "playerB_victories = 0\n",
    "playerA_possible_wins = 0\n",
    "playerB_possible_wins = 0\n",
    "playerA_missed_wins = 0\n",
    "playerB_missed_wins = 0\n",
    "playerA_possible_blocks = 0\n",
    "playerB_possible_blocks = 0\n",
    "playerA_missed_blocks = 0\n",
    "playerB_missed_blocks = 0\n",
    "playerA_created_possible_losses = 0\n",
    "playerB_created_possible_losses = 0\n",
    "playerA_starts = 0\n",
    "playerB_starts = 0\n",
    "playerA_middle_starts = 0\n",
    "playerB_middle_starts = 0\n",
    "playerA_firstplayerwins = 0\n",
    "playerB_firstplayerwins = 0\n",
    "tied_games=0\n",
    "gameboard_history = np.zeros((42,Xdim,Ydim),dtype=int) # use this to use gameboard_history to record all final gameboards\n",
    "playerA_distribution = [0]*Xdim\n",
    "playerB_distribution = [0]*Xdim\n",
    "#gameboard_history = np.zeros((test_games,Xdim,Ydim),dtype=int) # use this to use gameboard_history to record all final gameboards\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  sess.run(init)\n",
    "  ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "  saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "  print('Model loaded')\n",
    "        \n",
    "  play_yes_no = 'invalid'\n",
    "  done=0\n",
    "  game=0\n",
    "  while (play_yes_no[0] != 'n'):\n",
    "    while (play_yes_no[0] != 'n') and (play_yes_no[0] != 'y'):\n",
    "        play_yes_no = input('Would you like to play a game against the trained agent? (y/n)')\n",
    "    if (play_yes_no[0]=='n'):\n",
    "        print(\"OK, we'll play later.\") # No more games\n",
    "        break\n",
    "    else:    # if (play_yes_no[0]== 'y'):\n",
    "        print(\"\\nThe spots you play will be marked 1\\nThe spots the computer plays will be marked -1\\n\")\n",
    "        play_yes_no = 'invalid' # Loop back after the game to see if we play another game\n",
    "        gameboard = np.zeros((Xdim,Ydim),dtype=int) \n",
    "     \n",
    "        # reset gameboard\n",
    "        gameboard = np.zeros((Xdim,Ydim),dtype=int) \n",
    "        #gameboard_history = np.zeros((Xdim*Ydim,Xdim,Ydim),dtype=int) # Use this to use gameboard_history to record steps in a game\n",
    "        #observations = np.reshape(gameboard, [gameboard.shape[0], gameboard.shape[1], 1]) # unneeded?\n",
    "        #observation = np.concatenate([observations, observations, observations], axis=2)  # unneeded?\n",
    "        done = False\n",
    "           \n",
    "        playerA_first = (random.random()<0.5)\n",
    "        if playerA_first:\n",
    "            playerA_starts+=1\n",
    "        else:\n",
    "            playerB_starts+=1\n",
    "        episode_steps = 0    \n",
    "        while not done and 0 in gameboard:\n",
    "            \n",
    "            #Player A\n",
    "            if (episode_steps>0) or playerA_first:\n",
    "                gameboard *=-1 # switch sides every turn \n",
    "                episode_steps+=1\n",
    "                # Determine our move\n",
    "                column=playerA(gameboard)\n",
    "                playerA_distribution[column]+=1 # deleteme\n",
    "                #display_gameboard(gameboard) # deleteme\n",
    "                #print('trained', column) # deleteme\n",
    "                row = list(gameboard[column]).index(0) # Find the lowest open slot in the chosen column \n",
    "                gameboard[column,row] = 1 \n",
    "                # Update our stats\n",
    "                #done, playerA_victories, playerA_possible_wins, playerA_missed_wins, \\\n",
    "                    #playerA_possible_blocks, playerA_missed_blocks, playerA_created_possible_losses, playerA_middle_starts\\\n",
    "                    #= update_stats(gameboard,column, row,episode_steps, playerA_victories, playerA_possible_wins, playerA_missed_wins, \\\n",
    "                    #playerA_possible_blocks, playerA_missed_blocks, playerA_created_possible_losses, playerA_middle_starts)\n",
    "                #if done:\n",
    "                if check_for_win(gameboard,column,row):\n",
    "                    display_gameboard(gameboard)\n",
    "                    print(\"You won!\")\n",
    "                    gameboard_history[game] = gameboard\n",
    "                    if playerA_first:\n",
    "                        playerA_firstplayerwins +=1\n",
    "                    break # PlayerA won!\n",
    "        \n",
    "            # Check for a tie\n",
    "            if (0 not in gameboard):\n",
    "                tied_games+=1\n",
    "                gameboard_history[game] = gameboard\n",
    "                break\n",
    "            \n",
    "            #PlayerB        \n",
    "            gameboard *=-1 # switch sides every turn \n",
    "            episode_steps+=1\n",
    "            # Determine our move\n",
    "            #display_gameboard(gameboard) # deleteme\n",
    "            column=playerB(gameboard)\n",
    "            playerB_distribution[column]+=1 # deleteme\n",
    "            row = list(gameboard[column]).index(0) # Find the lowest open slot in the chosen column \n",
    "            gameboard[column,row] = 1 \n",
    "            # Update our stats\n",
    "            #done, playerB_victories, playerB_possible_wins, playerB_missed_wins, \\\n",
    "                #playerB_possible_blocks, playerB_missed_blocks, playerB_created_possible_losses, playerB_middle_starts\\\n",
    "                #= update_stats(gameboard,column, row,episode_steps, playerB_victories, playerB_possible_wins, playerB_missed_wins, \\\n",
    "                #playerB_possible_blocks, playerB_missed_blocks, playerB_created_possible_losses, playerB_middle_starts)\n",
    "            #if done:\n",
    "            if (check_for_win(gameboard,column,row)):\n",
    "                gameboard_history[game] = gameboard\n",
    "                if not playerA_first:\n",
    "                    playerB_firstplayerwins +=1\n",
    "                display_gameboard(-1*gameboard)\n",
    "                print('Too bad!')  # deleteme\n",
    "                break # PlayerB won!\n",
    "            \n",
    "            # Check for a tie\n",
    "            if (0 not in gameboard):\n",
    "                tied_games+=1\n",
    "                gameboard_history[game] = gameboard\n",
    "                break\n",
    "     \n",
    "        if (TRAIN_AFTER_TESTGAMES):\n",
    "            # At the end of each game, Get a random batch of experiences.\n",
    "            trainBatch = myBuffer.sample(batch_size) \n",
    "            # Below we perform the Double-DQN update to the target Q-values\n",
    "            Q1 = sess.run(mainQN.predict, feed_dict={mainQN.observation_input: np.stack(trainBatch[:,3], axis=0)})\n",
    "            Q2 = sess.run(targetQN.q_out, feed_dict={targetQN.observation_input: np.stack(trainBatch[:,3], axis=0)})\n",
    "            end_multiplier = -(trainBatch[:,4] - 1)\n",
    "            doubleQ = Q2[range(batch_size),Q1]\n",
    "            targetQ = trainBatch[:,2] + (targetQ_y*doubleQ * end_multiplier)\n",
    "            # Update the network with our target values.\n",
    "            _, q_loss = sess.run([mainQN.update, mainQN.loss],\n",
    "                feed_dict={mainQN.observation_input:np.stack(trainBatch[:,0], axis=0),\n",
    "                        mainQN.targetQ:targetQ, \n",
    "                        mainQN.actions:trainBatch[:,1]})\n",
    "            losses.append(q_loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
